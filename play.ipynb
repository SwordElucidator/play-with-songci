{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "VOCAB_DIR=\"generated/vocab\"\n",
    "VOCAB_FILE=\"songci.subword\"\n",
    "\n",
    "\n",
    "from train import training_loop\n",
    "from preprocessing.preprocessing import generate_data\n",
    "from model import ReformerLM\n",
    "import trax\n",
    "import numpy as np\n",
    "from trax import layers as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 5943\n",
      "input shape:  (8, 256)\n",
      "题：摸鱼儿 作：张炎 文：又孤吟、灞桥深雪，千山绝尽飞鸟。梅花也著东风笑，一夜瘦添多少。春悄悄。正断梦愁诗，忘却池塘草。前村路杳。看野水流冰，舟闲渡口，何必见安道。慵登眺。脉脉霏霏未了。寒威犹自清峭。终须几日开晴去，无奈此时怀抱。空暗恼。料酒兴歌情，未肯随人老。惜花起早。拚醉□忘归，接B565更好，一笑任倾倒。\n"
     ]
    }
   ],
   "source": [
    "train_stream, eval_stream, vocab_size = generate_data()\n",
    "\n",
    "print(\"vocab size:\", vocab_size)\n",
    "\n",
    "# the stream generators will yield (input, target, weights). let's just grab the input for inspection\n",
    "inp, _, _ = next(train_stream)\n",
    "\n",
    "# print the shape. format is (batch size, token length)\n",
    "print(\"input shape: \", inp.shape)\n",
    "\n",
    "# detokenize the first element\n",
    "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    Serial[\n",
      "      ShiftRight(1)\n",
      "    ]\n",
      "    Embedding_train_512\n",
      "    Dropout\n",
      "    PositionalEncoding\n",
      "    Dup_out2\n",
      "    ReversibleSerial_in2_out2[\n",
      "      ReversibleHalfResidual_in2_out2[\n",
      "        Serial[\n",
      "          LayerNorm\n",
      "        ]\n",
      "        SelfAttention\n",
      "      ]\n",
      "      ReversibleSwap_in2_out2\n",
      "      ReversibleHalfResidual_in2_out2[\n",
      "        Serial[\n",
      "          LayerNorm\n",
      "          Dense_2048\n",
      "          Dropout\n",
      "          Serial[\n",
      "            FastGelu\n",
      "          ]\n",
      "          Dense_512\n",
      "          Dropout\n",
      "        ]\n",
      "      ]\n",
      "      ReversibleSwap_in2_out2\n",
      "      ReversibleHalfResidual_in2_out2[\n",
      "        Serial[\n",
      "          LayerNorm\n",
      "        ]\n",
      "        SelfAttention\n",
      "      ]\n",
      "      ReversibleSwap_in2_out2\n",
      "      ReversibleHalfResidual_in2_out2[\n",
      "        Serial[\n",
      "          LayerNorm\n",
      "          Dense_2048\n",
      "          Dropout\n",
      "          Serial[\n",
      "            FastGelu\n",
      "          ]\n",
      "          Dense_512\n",
      "          Dropout\n",
      "        ]\n",
      "      ]\n",
      "      ReversibleSwap_in2_out2\n",
      "    ]\n",
      "    Concatenate_in2\n",
      "    LayerNorm\n",
      "    Dropout\n",
      "    Serial[\n",
      "      Dense_train\n",
      "    ]\n",
      "  ]\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "temp_model = ReformerLM('train')\n",
    "print(str(temp_model))\n",
    "\n",
    "# free memory\n",
    "del temp_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从头训练\n",
    "直接从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = training_loop()\n",
    "loop.run(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
    "\n",
    "def attention(*args, **kwargs):\n",
    "    kwargs['predict_mem_len'] = 120  # max length for predictions\n",
    "    kwargs['predict_drop_len'] = 120  # never drop old stuff\n",
    "    return tl.SelfAttention(*args, **kwargs)\n",
    "\n",
    "model = ReformerLM(\n",
    "    vocab_size=vocab_size,\n",
    "    n_layers=6,\n",
    "    mode='predict',\n",
    "    attention_type=attention,\n",
    ")\n",
    "\n",
    "\n",
    "# initialize from file\n",
    "model.init_from_file('model/model.pkl.gz',\n",
    "                     weights_only=True, input_signature=shape11)\n",
    "\n",
    "# save the starting state\n",
    "STARTING_STATE = model.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, vocab_file, vocab_dir):\n",
    "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
    "\n",
    "def detokenize(tokens, vocab_file, vocab_dir):\n",
    "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ReformerLM:  the Reformer language model you just trained\n",
    "        start_sentence (string): starting sentence of the conversation\n",
    "        vocab_file (string): vocabulary filename\n",
    "        vocab_dir (string): directory of the vocabulary file\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        generator: yields the next symbol generated by the model\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # Create input tokens using the the tokenize function\n",
    "    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)\n",
    "    \n",
    "    # Add batch dimension to array. Convert from (n,) to (x, n) where \n",
    "    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)\n",
    "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\n",
    "    \n",
    "    # call the autoregressive_sample_stream function from trax\n",
    "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
    "        # model\n",
    "        ReformerLM,\n",
    "        # inputs will be the tokens with batch dimension\n",
    "        inputs=input_tokens_with_batch,\n",
    "        # temperature\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return output_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ReformerLM:  the Reformer language model you just trained\n",
    "        model_state (np.array): initial state of the model before decoding\n",
    "        start_sentence (string): starting sentence of the conversation\n",
    "        vocab_file (string): vocabulary filename\n",
    "        vocab_dir (string): directory of the vocabulary file\n",
    "        max_len (int): maximum number of tokens to generate \n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        generator: yields the next symbol generated by the model\n",
    "    \"\"\"  \n",
    "    \n",
    "    # define the delimiters we used during training\n",
    "    delimiter_1 = '作：' \n",
    "    delimiter_2 = '文：'\n",
    "    \n",
    "    # initialize detokenized output\n",
    "    sentence = ''\n",
    "    \n",
    "    # token counter\n",
    "    counter = 0\n",
    "    \n",
    "    # output tokens. we insert a ': ' for formatting\n",
    "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
    "    \n",
    "    # reset the model state when starting a new dialogue\n",
    "    ReformerLM.state = model_state\n",
    "    \n",
    "    # calls the output generator implemented earlier\n",
    "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
    "    \n",
    "    # print the starting sentence\n",
    "    print(start_sentence.split(delimiter_2)[0].strip())\n",
    "    \n",
    "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
    "    for o in output:\n",
    "        \n",
    "        result.append(o)\n",
    "        \n",
    "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
    "        \n",
    "        if sentence.endswith(delimiter_1):\n",
    "            sentence = sentence.split(delimiter_1)[0]\n",
    "            print(f'{delimiter_2}{sentence}')\n",
    "            sentence = ''\n",
    "            result.clear()\n",
    "        \n",
    "        elif sentence.endswith(delimiter_2):\n",
    "            sentence = sentence.split(delimiter_2)[0]\n",
    "            print(f'{delimiter_1}{sentence}')\n",
    "            sentence = ''\n",
    "            result.clear()\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "        if counter > max_len:\n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = ' 题：'\n",
    "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
